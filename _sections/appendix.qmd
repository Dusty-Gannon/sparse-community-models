
## Appendix

## Reformulating the model {.appendix}

Our model can generally be written as

$$
{\bf y}_{t+1} = {\bf r} + u_m{\bf 1}_S + {\bf D}^{(\alpha)} + a\overset{\circ}{\bf H}N_t + \overset{\circ}{\bf A}N_t + {\bf X}_t^\text{(site)} \boldsymbol \beta + {\bf X}_t^\text{(spp)}\boldsymbol \gamma + {\bf W}_t{\boldsymbol \epsilon}_{t+1}
$$ {#eq-basemod-expanded}
where ${\bf y}_{t+1} = \big(\log (N_{1,t+1}/N_{1,t}),...,\log (N_{S,t+1}/N_{S,t})\big)^\top$ is a vector of log growth for each species, $1,2,...,S$. The vector $\bf r$ is an $S$-vector of intrinsic growth rates for each species, which is modified by the site effect $u_m \sim \mathcal{N}(0, \sigma_u^2)$ for site $m = 1,2,...,M$. We assume that the site effects are consistent across species, implying that some sites are simply more productive than others and that this generally benefits the growth of all species at the site. Site-level covariates can also be included in ${\bf X}_t^\text{site}$ to model fixed effects of sites, such as site-level treatments or environmental characteristics. 

The term ${\bf D}^{(\alpha)}$ is a diagonal matrix with the intra-specific competitive effects along the diagonal such that $D^{(\alpha)}_{ii}$ is the effect of increasing the density of species $i$ on itself. The term $\overset{\circ}{\bf H}$ is a *hollow* matrix (a matrix with zeros along the diagonal) with ones in all the off-diagonal elements such that $a\overset{\circ}{\bf H}$ is a hollow matrix with $a$, the *generic competitive effect*, in all off-diagonal elements. Thus, the parameter matrix $\overset{\circ}{\bf A}$ is also hollow and assumed sparse, and contains the deviations from the effect of a generic competitor. A non-generic competitor's effect on the growth of species $i$ will be $(a + \overset{\circ}{A_{ij}})N_{jt}$ for competitor $j\ne i$, while a generic competitor's effect will be $aN_{j,t}$ due to the assumed sparsity of $\overset{\circ}{\bf A}$.

Species-level covariates can be included in the model through the $S \times K$ matrix ${\bf X}_t^\text{(spp)}$, and additional interactive effects, such as alterations to a species competitive effect on others based on the environment at the site, can be included with interactions among these main effects. Finally, ${\bf W}_t$ is a diagonal weights matrix that scales the errors, ${\boldsymbol \epsilon}_1, {\boldsymbol \epsilon}_2,...,{\boldsymbol \epsilon}_T \overset{iid}{\sim} \mathcal{N}({\bf 0},\ {\bf I}_S)$, where ${\bf I}_S$ is the $S \times S$ identity matrix.

While there are many terms in @eq-basemod-expanded, this can be written quite succinctly as a *seemingly unrelated regressions* model by concatenating parameters into a single, high-dimensional vector and concatenating design matrices columnwise, padding with zeros as needed. Specifically,

$$
{\bf y} = \tilde{\bf X} \tilde{\boldsymbol \beta} + {\bf Z}{\bf u} + {\bf W}\boldsymbol \epsilon
$$ {#eq-basemod-condensed}
where ${\bf y} = \big({\bf y}^\top_{1,1},...,{\bf y}^\top_{1,M},{\bf y}^\top_{2,1},...,{\bf y}_{2,M}^\top,...,{\bf y}^\top_{T,M} \big)^\top$ is a vector of length $S \cdot M \cdot T$. The parameter vector $\tilde{\boldsymbol \beta}$ is a vector of all fixed effects, while $\bf u$ contains the random effects. As an example, assume there is a single, random site effect, $u_1, ..., u_M$, and the fixed effects include species-specific terms for biotic interactions along with intrinsic growth. In this instance,

$$
{\bf u} = \big(u_1, u_2, ..., u_M \big)^\top;\ \tilde{\boldsymbol \beta} = \big(a, {\bf r}^\top, \text{vec}({\bf A}^\top)^\top \big)^\top
$$
where $\text{vec}(\cdot)$ is the vectorization operator, vectorizing the non-hollow matrix ${\bf A} = {\bf D}_\alpha + \overset{\circ}{\bf A}$. Thus, the length of $\tilde{\boldsymbol \beta}$ will be $1 + S + S^2$. The design matrix, ${\tilde{\bf X}}$, which has dimensions $(S\cdot M \cdot T) \times (1 + S + S^2)$ can then be constructed by concatenating columns of the following matrices:

- ${\bf 1}_{S\cdot M \cdot T}$

- ${\bf 1}_{M\cdot T} \otimes {\bf I}_{S}$

- $\tilde{\bf N} = \begin{bmatrix} {\bf I}_S \otimes {\bf N}_{1,1}\\ {\bf I}_S \otimes {\bf N}_{2,1}\\ \vdots \\ {\bf I}_S \otimes {\bf N}_{M,T} \end{bmatrix}$

where $\otimes$ denotes the Kronecker product and ${\bf N}_{m,t}$ is the row-vector of species abundances at site $m$ at time step $t$. Similarly, the design matrix $\bf Z$ can be constructed as

- ${\bf Z} = \begin{bmatrix}({\bf I}_{M} \otimes {\bf 1}_S)_1\\ \vdots \\ ({\bf I}_{M} \otimes {\bf 1}_S)_T\end{bmatrix}$.

This then becomes a (potentially large) regression problem. 

## Variational approximations {.appendix}

The key to our variational approximation approach is proper book-keeping of the parameters that have all been stacked into $\tilde{\boldsymbol \beta}$. While all are, in essence, regression coefficients, some receive spike-slab shrinkage priors to shrink them towards zero [@mitchell_spikeslab_1988; @ishwaran_spikeslab_2005], while others receive weakly informative normal priors with mean 0 and standard deviation 2.5.

### Conditional posteriors and updates for regression coefficients {.appendix}

The first $S + 1$ parameters all receive weakly informative normal priors, and, because $\epsilon_{i,m,t}$ is normally-distributed for all $i,\ m,$ and $t$, the conditional posterior of $\tilde \beta_k$ is normal for $k = \{1, 2, ..., S+1\}$. Specifically, the posterior of $\tilde \beta_k$, conditional on all other parameters is
$$
p(\tilde \beta_k\ |\ {\bf y},\ {\boldsymbol \theta}_{-\tilde \beta_k}) = \mathcal{N}\left(\frac{{\bf e}_{-k}^\top {\bf W}^{-1} \tilde{\bf X}_k}{\tilde{\bf X}_k^\top {\bf W}^{-1} \tilde{\bf X}_k + \frac{1}{\tau^2}},\  \left(\tilde{\bf X}_k^\top {\bf W}^{-1} \tilde{\bf X}_k + \frac{1}{\tau^2}\right)^{-1}\right)
$$
where ${\boldsymbol \theta}_{-\tilde \beta_k}$ is the full set of parameters except for $\tilde \beta_k$, $\tilde {\bf X}_k$ is column $k$ of $\tilde {\bf X}$, and ${\bf e}_{-k}$ is a vector of partial residuals for explanatory variable $k$. That is, ${\bf e}_{-k} = ({\bf y} - \tilde {\bf X}_{-k}\tilde {\boldsymbol \beta}_{-k})$ where the $-k$ subscript indicates removing column and element $k$ from $\tilde \bf X$ and $\tilde {\boldsymbol \beta}$, respectively.

We can derive the variational approximation for $p(\tilde \beta_k\ |\ {\bf y},\ {\boldsymbol \theta}_{-\tilde \beta_k})$, $q(\tilde \beta_k\ |\ {\bf y},\ {\boldsymbol \theta}_{-\tilde \beta_k})$, through the identity that 
$$
q(\tilde \beta_k\ |\ {\bf y},\ {\boldsymbol \theta}_{-\tilde \beta_k}) = \exp\left\{\mathbb{E}_{-\tilde \beta_k}\big[\log p(\tilde \beta_k\ |\ {\bf y},\ {\boldsymbol \theta}_{-\tilde \beta_k})\big] \right\},
$$
where the expectation is taken over all parameters except $\tilde \beta_k$. This suggests an iterative approach where we update the parameters of each variational approximation, $q(\theta_i\ | {\boldsymbol \theta}_{-i})$, given the current parameters for the remaining set of conditional posteriors, in turn.

The variational update for the mean and standard deviation of $\tilde\beta_k$, conditional on the current values of all the other $q$, is therefore
$$
\mu_k^{(t+1)} = \frac{\mathbb{E}^{(t)}({\bf e}_{-k}^\top) \mathbb{E}^{(t)}( {\bf W}^{-1} ) \tilde{\bf X}_k}{\tilde{\bf X}_k^\top \mathbb{E}^{(t)}( {\bf W}^{-1} ) \tilde{\bf X}_k + \frac{1}{\tau^2}}
$$ {#eq-update-nonsprs-mu}
and
$$
s_k^{2^{(t+1)}} = \left(\tilde{\bf X}_k^\top \mathbb{E}^{(t)}( {\bf W}^{-1} ) \tilde{\bf X}_k + \frac{1}{\tau^2}\right)^{-1}
$$ {#eq-update-nonsprs-var}
where the $(t)$ superscript indicates the iteration of the coordinate ascent algorithm [@cavi-vb]. The hyperparameter $\tau^2$ is the variance of the of the normal prior placed on $\tilde \beta_k$. 

For regression coefficients that get spike-slab priors, we introduce an indicator variable, $\delta_k$ that takes that value of 1 with probability $\phi_k$ and 0 with probability $1 - \phi_k$, and multiply it by $\tilde \beta_k$ such that the prior for $\tilde \beta_k$ is the mixture model 

$$
\pi(\tilde \beta_k | \delta_k, {\bf W}) = \begin{cases}
\mathcal{N}(0, \tau^2) & \text{w.p. } \phi_k\\
0 & \text{w.p. } 1 - \phi_k
\end{cases}.
$$ {#eq-spike-slab-prior}

Thus, the update for the mean of the posterior of $\tilde \beta_k$ is

$$
\begin{split}
\mu_k = \frac{{\bf e}_{-k}^\top {\bf W}^{-1} \tilde{\bf X}_k}{\tilde{\bf X}_k^\top {\bf W}^{-1} \tilde{\bf X}_k + \frac{1}{\tau^2}} \mathbb{E} [\delta_k] \\
s_k^2 = \left(\tilde{\bf X}_k^\top {\bf W}^{-1} \tilde{\bf X}_k + \frac{1}{\tau^2}\right)^{-1}
\end{split}
$$ {#eq-update-sparse}

where $\mathbb{E}[\delta_k]$ is the current expectation of the indicator variable $\delta_k$. Note that these derivations are similar to those presented in @carbonetto_vb_2012, but with weights included. We also use simple conjugate priors and variational approximations for the error variances instead of the Importance Sampling approach proposed by @carbonetto_vb_2012. 

### Conditional posteriors and updates for latent indicators {.appendix}

The variational update for the latent variable $\delta_k$ is derived by setting $\delta_k = 1$ and $\delta_k = 0$ in turn. The update for $1 - P(\delta_k)$ can be shown to be

$$
1 - P(\delta_k) = 1 - \frac{1 - \phi_k}{(1 - \phi_k) + \phi_k\exp\left\{{\bf e}_{-k}^\top\mathbb{E}({\bf W}^{-1})\tilde{\bf X}_k\mathbb{E}(\tilde \beta_k) - \frac{1}{2}\bigg(\tilde{\bf X}_k^\top \mathbb{E}({\bf W}^{-1})\tilde{\bf X}_k + \tau^{-2}\bigg)\mathbb{E}(\tilde \beta^2) \right\}}
$$ {#eq-update-pip}

where the expectations are taken with respect to the current approximating distribution for each parameter. 

### Conditional posteriors and updates for variance components {.appendix}

Based on results in @gelman2006, we place half-student's $t$ priors on the scales for the variability of the random effects. Specifically, we let

$$
\begin{aligned}
u_m = \xi \psi_m\\
\psi_m \sim \mathcal{N}(0, s^2)\\
s^2 \sim \text{Inv-Gamma}(\nu/2, \nu/2) \\
\xi \sim \mathcal{N}(0, c^2)
\end{aligned}
$$
which induces a half-student's $t$ prior on the scale parameter $\sigma_u$ such that
$$
\begin{split}
u_1, u_2, ..., u_M \overset{iid}{\sim} \mathcal{N}(0, \sigma_u^2)\\
\sigma_u \sim t^{+}(\nu, c)
\end{split}
$$
where $\nu$ is the degrees of freedom parameter and $c$ is the scale parameter for the prior on $\sigma_u$. This is known as the *parameter expanded* prior hierarchy [@par-expand-half-t], and is useful in this case because the priors on $\{\psi_1,\psi_2,...,\psi_M, s^2, \xi\}$ are all conditionally conjugate. We can make use of this fact to derive the variational updates.

<!-- Need to check this! -->
The conditional posterior of $\xi$ is
$$
p(\xi | \boldsymbol{\theta}_{-\xi}, {\bf y}) = \mathcal{N}\left(\frac{{\bf r}^\top {\bf W}^{-1} {\bf Z} \boldsymbol \psi}{\boldsymbol \psi^\top {\bf Z}^\top {\bf W}^{-1} {\bf Z} \boldsymbol \psi + c^{-2}},\ \frac{1}{\boldsymbol \psi^\top {\bf Z}^\top {\bf W}^{-1} {\bf Z} \boldsymbol \psi + c^{-2}} \right)
$$
where $\bf r$ is the vector of marginal residuals, $r_i = (y_i - \tilde{\bf X}_i^\top \tilde{\boldsymbol \beta})$. We can therefore derive the variational updates by
$$
q(\xi|\boldsymbol \theta_{-\xi},{\bf y}) = \exp\left\{\mathbb{E}_{-\xi}\left[ \log p(\xi\ |\ \boldsymbol \theta_{-\xi},{\bf y}) \right]\right\}
$$
where $\mathbb{E}_{-\xi}$ denotes the expectation taken over all parameters *except* $\xi$. This yields the updates
$$
\hat\mu_\xi^{(t+1)} = \frac{\mathbb{E}^{(t)}({\bf r})^\top \mathbb{E}^{(t)}({\bf W}^{-1}) {\bf Z}\ \mathbb{E}^{(t)}(\boldsymbol \psi)}{\mathbb{E}^{(t)}({\boldsymbol \psi}^\top {\bf Z}^\top {\bf W}^{-1} {\bf Z} \boldsymbol \psi) + c^{-2}}
$$ {#eq-update-xi-mean}
and
$$
\hat{v}_{\xi}^{(t+1)} = \left(\mathbb{E}^{(t)}({\boldsymbol \psi}^\top {\bf Z}^\top {\bf W}^{-1} {\bf Z} \boldsymbol \psi) + c^{-2} \right)^{-1}
$$ {#eq-update-xi-var}
where $\mathbb{E}^{(t)}$ denotes the expected value of the current conditional posterior and a superscript of $(t+1)$ indicates an update to the parameter of interest on the next iteration of the algorithm.

Similarly, the conditional posterior for $\psi_m$ is

$$
p(\psi_m\ |\ {\boldsymbol \theta}_{-\psi_m},\ {\bf y}) = \mathcal{N}\left(\frac{{\bf e}_{-\psi_m}^\top {\bf W}^{-1} {\bf Z}_m \xi}{{\bf Z}_m^\top {\bf W}^{-1} {\bf Z}_m \xi^2 + s^{-2}},\ \frac{1}{{\bf Z}_m^\top {\bf W}^{-1} {\bf Z}_m \xi^2 + s^{-2}}\right)
$$
where ${\bf e}_{-\psi_m}$ is the vector of (partial) conditional residuals without the adjustment to the mean due to $\xi\psi_m$. This leads to the update equations

$$
\hat{\mu}_{\psi_m}^{(t+1)} = \frac{\mathbb{E}^{(t)}({\bf e}_{-\psi_m})^\top \mathbb{E}^{(t)}({\bf W}^{-1}) {\bf Z}_m \mathbb{E}^{(t)}(\xi)}{{\bf Z}_m^\top \mathbb{E}^{(t)}({\bf W}^{-1}) {\bf Z}_m \mathbb{E}^{(t)}(\xi^2) + \mathbb{E}(s^{-2})}
$$ {#eq-update-psi-mean}
and
$$
\hat{v}_{\psi_m}^{(t+1)} = \left( {\bf Z}_m^\top \mathbb{E}^{(t)}({\bf W})^{-1} {\bf Z}_m \mathbb{E}^{(t)}(\xi^2) + \mathbb{E}^{(t)}(s^{-2}) \right)^{-1}.
$${#eq-update-psi-var}

The conditional posterior of $s^2$ is also Inverse-Gamma, with shape parameter
<!-- Double check this!!! -->
$$
a_{s} = \frac{\nu}{2} + K
$$
and rate parameter
$$
b_s = \frac{1}{2}\left(\nu + \sum_j \psi_j^2\right).
$$
The shape parameter is therefore constant across the updates, but the rate parameter updates are given by
$$
b_s^{(t+1)} = \frac{1}{2}\nu + \frac{1}{2}\sum_j \mathbb{E}^{(t)}(\psi_j^2).
$${#eq-update-s2-rate}

Finally, we can update the error variances, which, for $\sigma_1^2,\sigma_2^2,...,\sigma_S^2$ we give improper Jeffrey's priors of $1/\sigma_i^2$ for $i = 1,2,...,S$. The conditional posterior is therefore inverse gamma, with constant shape parameter $n_s/2 + 1$ and updates to the rate parameter of

$$
b_{\sigma_i}^{(t+1)} = \frac{1}{2}\mathbb{E}\left(({\bf y}_{[i]} - \tilde{\bf X}_{[i]}\tilde{\boldsymbol \beta} - {\bf Z}_{[i]}{\boldsymbol \psi}\xi)^\top \tilde{\bf W}_{[i]}({\bf y}_{[i]} - \tilde{\bf X}_{[i]}\tilde{\boldsymbol \beta} - {\bf Z}_{[i]}{\boldsymbol \psi}\xi) \right)
$$

where the subscript ${[i]}$ indicates that the data vectors and model matrices are subsets to just the rows in which the response is growth of species $i$. The matrix $\tilde {\bf W}$ is a matrix with known *inverse* weights and the vector of error variances factored out. For example, if the weight for observation $y_{i,t}$ is $1/\sqrt{N_{i,t-1}}$, where $N_{i,t}$ is the density or size of the population of species $i$ at time $t$, then $w_{i,t} = \sigma_i^2/{N_{i, t - 1}}$ and $\tilde w_i = N_{i,t}$.


