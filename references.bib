@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}


% ----------- Bayesian learning ---------------


@article{carbonetto_vb_2012,
	title = {Scalable {Variational} {Inference} for {Bayesian} {Variable} {Selection} in {Regression}, and {Its} {Accuracy} in {Genetic} {Association} {Studies}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-7/issue-1/Scalable-Variational-Inference-for-Bayesian-Variable-Selection-in-Regression-and/10.1214/12-BA703.full},
	doi = {10.1214/12-BA703},
	abstract = {The Bayesian approach to variable selection in regression is a powerful tool for tackling many scientific problems. Inference for variable selection models is usually implemented using Markov chain Monte Carlo (MCMC). Because MCMC can impose a high computational cost in studies with a large number of variables, we assess an alternative to MCMC based on a simple variational approximation. Our aim is to retain useful features of Bayesian variable selection at a reduced cost. Using simulations designed to mimic genetic association studies, we show that this simple variational approximation yields posterior inferences in some settings that closely match exact values. In less restrictive (and more realistic) conditions, we show that posterior probabilities of inclusion for individual variables are often incorrect, but variational estimates of other useful quantities{\textbar}including posterior distributions of the hyperparameters{\textbar}are remarkably accurate. We illustrate how these results guide the use of variational inference for a genome-wide association study with thousands of samples and hundreds of thousands of variables.},
	number = {1},
	urldate = {2022-11-21},
	journal = {Bayesian Analysis},
	author = {Carbonetto, Peter and Stephens, Matthew},
	month = mar,
	year = {2012},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Variable selection, genetic association studies, Monte Carlo, variational inference},
	pages = {73--108},
	file = {Full Text PDF:/Users/dustygannon/Zotero/storage/PQZ43A5V/Carbonetto and Stephens - 2012 - Scalable Variational Inference for Bayesian Variab.pdf:application/pdf;Snapshot:/Users/dustygannon/Zotero/storage/GZ9PFFIM/12-BA703.html:text/html},
}


@article{ishwaran_spikeslab_2005,
	title = {Spike and slab variable selection: {Frequentist} and {Bayesian} strategies},
	volume = {33},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Spike and slab variable selection},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-2/Spike-and-slab-variable-selection-Frequentist-and-Bayesian-strategies/10.1214/009053604000001147.full},
	doi = {10.1214/009053604000001147},
	abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure’s ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.},
	number = {2},
	urldate = {2022-10-15},
	journal = {The Annals of Statistics},
	author = {Ishwaran, Hemant and Rao, J. Sunil},
	month = apr,
	year = {2005},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62J07, 62J05, Generalized ridge regression, hypervariance, model averaging, model uncertainty, ordinary least squares, Penalization, rescaling, shrinkage, stochastic variable selection, Zcut},
	pages = {730--773},
	file = {Full Text PDF:/Users/dustygannon/Zotero/storage/FZVREWZV/Ishwaran and Rao - 2005 - Spike and slab variable selection Frequentist and.pdf:application/pdf;Snapshot:/Users/dustygannon/Zotero/storage/R6XBHBF4/009053604000001147.html:text/html},
}


@article{mitchell_spikeslab_1988,
	title = {Bayesian {Variable} {Selection} in {Linear} {Regression}},
	volume = {83},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478694},
	doi = {10.1080/01621459.1988.10478694},
	abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a “spike and slab” distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose y. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of “leave one out” approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
	number = {404},
	urldate = {2022-11-03},
	journal = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	month = dec,
	year = {1988},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478694},
	keywords = {Cross-validation, Linear models, Subset selection},
	pages = {1023--1032},
}



